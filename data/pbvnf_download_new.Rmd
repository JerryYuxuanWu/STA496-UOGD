---
title: "pbvnf-download-new"
author: "William Zhang, Jerry"
date: "2024-09-04"
output: html_document
---
```{r}
library(httr)
library(jsonlite)
library(utils)
library(tidyverse)
library(lubridate)
library(dagR)
library(R.utils)
library(data.table)
library(leaflet)
```

```{r}
# compute the coordinates for that is distance (km) away at given bearing.
new.lon.lat <-
function (lon, lat, bearing, distance) 
{
    rad <- pi/180
    a1 <- lat * rad
    a2 <- lon * rad
    tc <- bearing * rad
    d <- distance/6378.145
    nlat <- asin(sin(a1) * cos(d) + cos(a1) * sin(d) * cos(tc))
    dlon <- atan2(sin(tc) * sin(d) * cos(a1), cos(d) - sin(a1) * 
        sin(nlat))
    nlon <- ((a2 + dlon + pi)%%(2 * pi)) - pi
    npts <- cbind(nlon/rad, nlat/rad)
    return(npts)
}
```

```{r}
# Feb 3, 2025, modifying the boundary to take flaring measurements
source <- c(32.2961, -104.1089)
north_bound <- new.lon.lat(source[2], source[1], 0, 200)
south_bound <- new.lon.lat(source[2], source[1], 180, 200)
west_bound <- new.lon.lat(source[2], source[1], 270, 200)
east_bound <- new.lon.lat(source[2], source[1], 90, 200)

# Sep 4, 2024: ~20km around furthest samplers
nw_bound <- c(32.624006, -104.512270)
sw_bound <- c(32.117624, -104.512270)
se_bound <- c(32.117624, -103.903443)
ne_bound <- c(32.624006, -103.903443)
```

```{r}
# have a look at the boundaries
bound_df <- rbind(north_bound, south_bound, west_bound, east_bound) %>%
  as_tibble() %>%
  set_names(c('lon', 'lat'))

# load in data for 10km around the loving site
# pb_vnf_20120301_20240423 <- readRDS('pb_vnf/pb-vnf-20120301-20240423.rds')

bound_df %>%
  leaflet() %>%
  addTiles() %>%
  addCircleMarkers(lng = ~lon, lat = ~lat) %>%
  addCircleMarkers(lng = source[2], lat = source[1], color = 'red') # %>%
  # addCircleMarkers(lng = pb_vnf_20120301_20240423$lon,
  #                  lat = pb_vnf_20120301_20240423$lat,
  #                  radius = 3, stroke = F, fillColor = 'black')
```

```{r}
vnf.cols = c('date_mscan', 'date_ltz', 'lon_gmtco', 'lat_gmtco', 'temp_bb', 'temp_bkg',
               'esf_bb', 'rhi', 'rh', 'area_pixel', 'area_bb', 'cloud_mask')
# square boundary taken to be 200km from the loving's site

# old limits
# lon_lim <- c(-104.8058, -100.6712)
# lat_lim <- c(30.63064, 33.6984)

# new limits
# lon: (-105.1716 -103.0462)
# lat: (31.39779 33.19441)
lon_lim <- c(west_bound[1], east_bound[1])
lat_lim <- c(south_bound[2], north_bound[2])
```

# Downloading data loop
```{r}
# Retrieve access token
params <- list(
client_id = 'eogdata_oidc',
client_secret = '2677ad81-521b-4869-8480-6d05b9e57d48',
username = 'jerryp.wu@mail.utoronto.ca',
password = 'kBREu_D63qZ34g4',
grant_type = 'password'
)
token_url <- 'https://eogauth.mines.edu/auth/realms/master/protocol/openid-connect/token'
response <- POST(token_url, body = params, encode = "form")
access_token_list <- fromJSON(content(response,as="text",encoding="UTF-8"))
access_token <- access_token_list$access_token
auth <- paste('Bearer', access_token)

# In case there's no previous data, start from scratch
vnf <- data.table(
    vnf_id = character(),
    time_scan =  as.Date(character()),
    date = as.Date(character()),
    lon = numeric(),
    lat = numeric(),
    temp_bb = integer(),
    temp_bkg = integer(),
    esf_bb = numeric(),
    rhi = numeric(),
    rh = numeric(),
    area_pixel = numeric(),
    area_bb = numeric(),
    cloud_mask = integer(),
    file_date = as.Date(character())
  )

start_date = as.Date('2023-04-14')

# if there is previous data, load in to update
# previous_data <- readRDS('pb_vnf/pb-vnf_220831-240301-v1.rds')
# 
# start_date = max(previous_data$file_date) + 1

# Set end date

end_date = as.Date("2024-09-01")

new_file_name <- 'loving200km-vnf_230414-240901.rds'

# vector of dates
if(start_date > end_date){
  stop("Invalid dates for update.")
} else{
  vnf.dates = seq(start_date, end_date, "days")
}

vnfv30.url.pfx <- c(
  "https://eogdata.mines.edu/wwwdata/viirs_products/vnf/v30//VNF_npp_d",
  "_noaa_v30-ez.csv.gz")

# Download & update loop --------------------------------------------------
for(d in 1:length(vnf.dates)){
  # prep ----
  # starts timer to track download time
  start_time = Sys.time()
  
  # V30 GRAVITE version
  url.name = paste0(vnfv30.url.pfx[1], 
                    gsub('-', '', vnf.dates[d]), vnfv30.url.pfx[2])
  gz.name = paste0('loving_vnf/raw/', basename(url.name))
  
  # get CSV name for unzipped file
  csv.name = gsub(".gz", "", gz.name)
  
  # download & process ----
  # try to download the file, error handler in case of no data
  tryCatch({
    # message indicating date being downloaded
    cat("[", as.character(vnf.dates[d]),'] download. ', sep='')
    
    # downloads the file ----
    download.file(url.name, gz.name, 
                  mode = 'wb', quiet = TRUE,
                  headers = list(Authorization = auth))
    # unzips GZ, keeps the CSV, removes the GZ
    R.utils::gunzip(gz.name, overwrite = TRUE)
    # process
    cat('process. ', sep='')
    
    # process the file ----
    vnf.temp = fread(csv.name) %>% 
      rename_all(tolower) %>% # rename all columns to lowercase for convenience
      select(all_of(vnf.cols)) %>% # collect relevant columns
      mutate(across(where(is.numeric),  ~ na_if(.x, 999999))) %>% # replace 999999 as missing
      filter(!is.na(temp_bb)) %>% # keep those not missing temperature
      # generate unique VNF_ID and valid date-format date column
      mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[d]), sprintf('%06d', 1:nrow(.))),
             date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
             file_date = vnf.dates[d]) %>% 
      # collect and rename certain columns, and drop a few
      select(vnf_id, time_scan = date_ltz, date, lon = lon_gmtco, lat = lat_gmtco,
             everything(), -date_mscan) %>% 
      # filter by lat & lon bounds (bbox)
      filter(lon >= lon_lim[1], lon <= lon_lim[2],
             lat >= lat_lim[1], lat <= lat_lim[2])
    
    # delete CSV file
    file.remove(csv.name)
    
    # report time took t to download and process ----
    cat(difftime(Sys.time(), start_time, units='secs') %>% ceiling(),
        " secs.\r", sep='')
  }, error = function(e){
    print(e)
  })
  
  # append to existing data
  vnf <- bind_rows(vnf, vnf.temp)
  
  # output every 50 days. updating every day would be pretty slow
  if(d %% 50 == 0 | d == length(vnf.dates)){
    saveRDS(vnf, paste0("loving_vnf/", new_file_name))
  }
}
```



